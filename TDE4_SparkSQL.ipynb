{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK+z0Si1/YdAn6rHpLmECm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KurlenMurlen/BigDataAssignment3/blob/main/TDE4_SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### Group:\n",
        "- Tarso Bertolini Rodrigues\n",
        "- Murilo Chandelier\n",
        "- Ricardo Ryu\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "You and your team were hired to process data using Apache Spark SQL. The team have to answer the 10 (ten) questions (tasks) defined in the activity TDE 2, 3, and 4 - Dataset Definition.\n",
        "\n",
        "---\n",
        "\n",
        "Given the aforementioned context, you are in charge of developing a set of solutions that allow the company to answer the following demands:\n",
        "\n",
        "(Easy) The first task evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "(Easy) The second task Evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "(Easy) The third task Evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "(Easy) The fourth task Evolving simple aggregations without the requirement of custom data types;\n",
        "\n",
        "(Medium) The first task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "(Medium) The second task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "(Medium) The third task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "(Medium) The fourth task Containing both Custom Keys, Custom Values, and Combining;\n",
        "\n",
        "(Hard) The first task Containing the use of multiple jobs, Custom Keys, Custom Values, Combining to solve the tasks;\n",
        "\n",
        "(Hard) The second task Containing the use of multiple jobs, Custom Keys, Custom Values, Combining to solve the tasks;\n",
        "\n",
        "---\n",
        "\n",
        "Given your knowledge and skills in Python and Apache Spark SQL, for each item above, provide:\n",
        "\n",
        "The source code for solving the problem using Apache Spark SQL programming\n",
        "The result of your code run in a separate text file (.txt). If more than 5 rows of results are available, you must report only the 5 first rows of such result.\n",
        "Important:\n",
        "\n",
        "The grading of this activity is conditioned to the audit test.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LL1xGDmWgaLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing and Importing SparkSQL"
      ],
      "metadata": {
        "id": "0b2CIujYk1oy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-H1d1iHtenTE"
      },
      "outputs": [],
      "source": [
        "# Install Spark in Google Colab\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\"\n",
        "\n",
        "# Initialize Spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"SuicidioDataAnalysis\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Loading the CSV Data"
      ],
      "metadata": {
        "id": "e1nn5ukTldJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from CSV\n",
        "file_path = \"suicidios_2010_a_2019.csv\"  # Change this path to your file location in Colab\n",
        "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "data.createOrReplaceTempView(\"suicidios_data\")"
      ],
      "metadata": {
        "id": "6V1BgjGDljf9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Basic Queries\n",
        "\n",
        "\n",
        "```\n",
        "output_basic_raceColor = \"output_basic_raceColor\"\n",
        "output_basic_gender = \"output_basic_gender\"\n",
        "output_basic_states = \"output_basic_states\"\n",
        "output_basic_occupationsDistributions = \"output_basic_occupationsDistributions\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "rhyBrsknl5hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_basic_raceColor = \"output_basic_raceColor\"\n",
        "df_raceColor = spark.sql(\"SELECT RACACOR, COUNT(*) AS count FROM suicidios_data GROUP BY RACACOR\")\n",
        "df_raceColor.show()\n",
        "df_raceColor.write.mode(\"overwrite\").csv(output_basic_raceColor)"
      ],
      "metadata": {
        "id": "1JiecF7-mXep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_basic_gender = \"output_basic_gender\"\n",
        "df_gender = spark.sql(\"SELECT SEXO, COUNT(*) AS count FROM suicidios_data GROUP BY SEXO\")\n",
        "df_gender.show()\n",
        "df_gender.write.mode(\"overwrite\").csv(output_basic_gender)"
      ],
      "metadata": {
        "id": "ZwVmGXTjmlCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_basic_states = \"output_basic_states\"\n",
        "df_states = spark.sql(\"SELECT estado, COUNT(*) AS count FROM suicidios_data GROUP BY estado\")\n",
        "df_states.show()\n",
        "df_states.write.mode(\"overwrite\").csv(output_basic_states)"
      ],
      "metadata": {
        "id": "m4hDlC0jmqj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_basic_occupationsDistributions = \"output_basic_occupationsDistributions\"\n",
        "df_occupations = spark.sql(\"SELECT OCUP, COUNT(*) AS count FROM suicidios_data GROUP BY OCUP\")\n",
        "df_occupations.show()\n",
        "df_occupations.write.mode(\"overwrite\").csv(output_basic_occupationsDistributions)"
      ],
      "metadata": {
        "id": "B0o3kRhJmsH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Intermediate Queries\n",
        "\n",
        "\n",
        "```\n",
        "output_intermediary_genderYear = \"output_intermediary_genderYear\"\n",
        "output_intermediary_genderState = \"output_intermediary_genderState\"\n",
        "output_intermediary_occupationsDistributionsState = \"output_intermediary_occupationsDistributionsState\"\n",
        "output_intermediary_dateOfBirthState = \"output_intermediary_dateOfBirthState\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "uDVXyDgcmzFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_intermediary_genderYear = \"output_intermediary_genderYear\"\n",
        "df_genderYear = spark.sql(\"SELECT ano, SEXO, COUNT(*) AS count FROM suicidios_data GROUP BY ano, SEXO\")\n",
        "df_genderYear.show()\n",
        "df_genderYear.write.mode(\"overwrite\").csv(output_intermediary_genderYear)"
      ],
      "metadata": {
        "id": "IQiAsdPzm-_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_intermediary_genderState = \"output_intermediary_genderState\"\n",
        "df_genderState = spark.sql(\"SELECT estado, SEXO, COUNT(*) AS count FROM suicidios_data GROUP BY estado, SEXO\")\n",
        "df_genderState.show()\n",
        "df_genderState.write.mode(\"overwrite\").csv(output_intermediary_genderState)"
      ],
      "metadata": {
        "id": "MN2ZduLunOHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_intermediary_occupationsDistributionsState = \"output_intermediary_occupationsDistributionsState\"\n",
        "df_occupationsState = spark.sql(\"SELECT estado, OCUP, COUNT(*) AS count FROM suicidios_data GROUP BY estado, OCUP\")\n",
        "df_occupationsState.show()\n",
        "df_occupationsState.write.mode(\"overwrite\").csv(output_intermediary_occupationsDistributionsState)"
      ],
      "metadata": {
        "id": "BK2BCnOvnQ2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_intermediary_occupationsDistributionsState = \"output_intermediary_occupationsDistributionsState\"\n",
        "df_occupationsState = spark.sql(\"SELECT estado, OCUP, COUNT(*) AS count FROM suicidios_data GROUP BY estado, OCUP\")\n",
        "df_occupationsState.show()\n",
        "df_occupationsState.write.mode(\"overwrite\").csv(output_intermediary_occupationsDistributionsState)"
      ],
      "metadata": {
        "id": "idv1a8jWnTlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Advanced Queries\n",
        "\n",
        "\n",
        "```\n",
        "output_advanced_assistanceCorrelation = \"output_advanced_assistanceCorrelation\"\n",
        "output_advanced_occupationCause = \"output_advanced_occupationCause\"\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Y73wcRg2nvNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_advanced_assistanceCorrelation = \"output_advanced_assistanceCorrelation\"\n",
        "df_assistanceCorrelation = spark.sql(\"\"\"\n",
        "    SELECT estado, ASSISTMED, COUNT(*) AS count\n",
        "    FROM suicidios_data\n",
        "    GROUP BY estado, ASSISTMED\n",
        "\"\"\")\n",
        "df_assistanceCorrelation.show()\n",
        "df_assistanceCorrelation.write.mode(\"overwrite\").csv(output_advanced_assistanceCorrelation)"
      ],
      "metadata": {
        "id": "oTcu6tGhn5Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_advanced_occupationCause = \"output_advanced_occupationCause\"\n",
        "df_occupationCause = spark.sql(\"\"\"\n",
        "    SELECT OCUP, CAUSABAS, COUNT(*) AS count\n",
        "    FROM suicidios_data\n",
        "    GROUP BY OCUP, CAUSABAS\n",
        "\"\"\")\n",
        "df_occupationCause.show()\n",
        "df_occupationCause.write.mode(\"overwrite\").csv(output_advanced_occupationCause)"
      ],
      "metadata": {
        "id": "wQ3JwJ_3n7wb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}